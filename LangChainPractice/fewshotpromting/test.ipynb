{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader,TextLoader\n",
    "from langchain.prompts import SemanticSimilarityExampleSelector\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import SemanticSimilarityExampleSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('data/examples/', glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Splitting the document into chunks and creating splits\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200,add_start_index=True)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Open AI Embeddings\n",
    "embeddings=OpenAIEmbeddings(api_key=os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the Vector data store\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating an example selector\n",
    "example_selector = SemanticSimilarityExampleSelector(\n",
    "    vectorstore=vectorstore,\n",
    "    k=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': 'data\\\\examples\\\\AWS_Lambda_Python.txt', 'start_index': 126},\n",
       " {'source': 'data\\\\examples\\\\AWS_Lambda_Python.txt', 'start_index': 0}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The prompt template will load examples by passing the input do the `select_examples` method\n",
    "example_selector.select_examples({\"input\": \"Lambda_Python\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    FewShotChatMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "# Define the few-shot prompt.\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    # The input variables select the values to pass to the example_selector\n",
    "    input_variables=[\"input\"],\n",
    "    example_selector=example_selector,\n",
    "    # Define how each example will be formatted.\n",
    "    # In this case, each example will become 2 messages:\n",
    "    # 1 human, and 1 AI\n",
    "    example_prompt=ChatPromptTemplate.from_messages(\n",
    "        [(\"human\", \"{input}\"), (\"ai\", \"{output}\")]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a wondrous wizard of math.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)\n",
    "\n",
    "retriever = db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='The below points should be considered while converting AWS Lambda Function \\nto GCP Cloud Function and vice versa.  \\nEvent Sources:  \\n• AWS Lambda:  Supports SQS, SNS, S3, Kafka, CloudWatch Events, \\nDynamoDB, Kinesis, and more.', metadata={'page': 0, 'source': 'data\\\\knowledge_base\\\\ConvertAWSLambaToGCPCloudFunctionViceVersa.pdf'}),\n",
       " Document(page_content='Cloud Functions runtime.  \\nCode Libraries and Dependencies:  \\n• AWS Lambda:  Leverages AWS SDK for interaction with other AWS services.  \\n• GCP Cloud Functions:  Uses GCP client libraries for interacting with GCP \\nservices.', metadata={'page': 0, 'source': 'data\\\\knowledge_base\\\\ConvertAWSLambaToGCPCloudFunctionViceVersa.pdf'}),\n",
       " Document(page_content=\"• GCP Cloud Functions:  Node.js, Python, Go, Java, PHP, Ruby  \\no Conversion:  Check runtime compatibility. If your Lambda function uses \\nPowerShell or .NET Core, you'll need to rewrite it in a supported GCP \\nCloud Functions runtime.\", metadata={'page': 0, 'source': 'data\\\\knowledge_base\\\\ConvertAWSLambaToGCPCloudFunctionViceVersa.pdf'}),\n",
       " Document(page_content='GCP service. Similar to boto3, these libraries enable your Cloud Functions to \\ninteract with GCP services. Examples include the Cloud Storage library for \\nmanaging object storage, the Cloud Pub/Sub library for message queuing, and', metadata={'page': 1, 'source': 'data\\\\knowledge_base\\\\ConvertAWSLambaToGCPCloudFunctionViceVersa.pdf'})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"CWhat are the specific dependencies or libraries used in the AWS Lambda function that need to be considered during the conversion to GCP Cloud Function?\")\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "system = \"\"\"You are a code conversion tool that converts code for a specific service from one cloud platform to another service on a different cloud platform.\n",
    "\n",
    "You will be asked to convert source cloud service code to target cloud service code.\n",
    "\n",
    "Given a specific user request to convert the code, write a more generic question that needs to be answered in order to know the specific details needed for conversion. \\\n",
    "\n",
    "If you don't recognize a word or acronym to not try to rewrite it.\n",
    "\n",
    "Write concise questions.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "step_back = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the specific dependencies or libraries used in the AWS Lambda function that need to be considered during the conversion to GCP Cloud Function?\n"
     ]
    }
   ],
   "source": [
    "question = (\n",
    "    \"Covert the given AWS lambda function written in Python to GCP Cloud Function in Python.\"\n",
    ")\n",
    "result = step_back.invoke({\"question\": question})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Click Create. Step 4: Attach the Layer to Your Lambda Function. Finally, attach the newly created layer to your Lambda function: 1. Open your Lambda function in the AWS console. 2. Scroll to the \"Layers\" section in the function\\'s designer. 3. Click \"Add a layer\". 1. AWS Lambda layers allow you to reuse code and dependencies across multiple Lambda functions. This helps reduce code duplication, manage dependencies better, and decrease the size of your Lambda ... To use it in a new Lambda function, follow these steps: Go to the AWS Lambda service and click on \"Create Function\". Provide a name for your function and set the runtime to \"Python 3.9 ... A nodejs folder is auto-generated for you. In there you\\'ll find an empty package.json file and a node_modules folder. If you want to offload other node_modules you can either: cd into the nodejs folder and add the dependencies into the package.json file, or; move all your existing function\\'s node_modules content into the layer\\'s node_modules folder; Any dependency listed within the layer\\'s ...'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vivprasad\\Desktop\\Mine\\DataScienceCourse\\LangChainPractice\\RAG_Langchain\\myenv\\lib\\site-packages\\curl_cffi\\aio.py:205: UserWarning: Curlm alread closed! quitting from process_data\n",
      "  warnings.warn(\"Curlm alread closed! quitting from process_data\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
    "\n",
    "search = DuckDuckGoSearchAPIWrapper(max_results=4)\n",
    "\n",
    "def retriever(query):\n",
    "    return search.run(query)\n",
    "\n",
    "######################################\n",
    "\n",
    "retriever(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
